{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYme1YaAdWxD"
   },
   "source": [
    "# ANIMAL HUBANDRY LLM\n",
    "\n",
    "# Fine-tuning Nous-Hermes-Llama2-13b on your own data for domain specific task\n",
    "\n",
    "In this notebook and tutorial, we will fine-tune  [Nous-Hermes-Llama2-13b](https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b). which is a variant variant of llama 2\n",
    "\n",
    "Reason for fintuning Nous-Hermes-Llama2-13b is that it stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms.\n",
    "\n",
    "This tutorial will using gradient ai, to hundle our computaional resources and host the model.\n",
    "\n",
    "Gradient ai is a platform that allows you to easily fine-tune existiong base model using API.\n",
    "\n",
    "\n",
    "You can check out the full project on our githubpage: [animal_husbandry_LLM](https://github.com/ice-black/animal_husbandry_LLM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtQIYQI1dWxc"
   },
   "source": [
    "## Let's begin!\n",
    "\n",
    "The purpose is to fine-tune our base language model specifically for the domain of animal husbandry **(question answering)**\n",
    "\n",
    "The packages used  are :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuXIFTFapAMI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f931312c-f7e2-4b8d-cf87-45d012c50a7b",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install gradientai --upgrade\n",
    "!pip install langchain\n",
    "!pip install -U gradient_haystack\n",
    "!pip install regex\n",
    "!pip install rouge\n",
    "!pip install nltk\n",
    "!pip install wandb\n",
    "!pip install datasets\n",
    "!pip install rouge\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcE4NTeFyRgd"
   },
   "source": [
    "# **1. LOAD DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uUnvd_fdWxv"
   },
   "source": [
    "** Preparing data**\n",
    "To prepare the dataset for fine-tuning, you'll need to structure it as a `.json` file with input-output pairs. Each pair should be formatted as follows\n",
    "```\n",
    "{ \"inputs\": \"<s>### Instruction:\\n  {user_query} \\n\\n### Response:\\n {response} </s>\"}\n",
    "```\n",
    "Here's an example of how our dataset looks like:\n",
    "\n",
    "```\n",
    "{ \"inputs\": \"<s>### Instruction:\\nDiscuss the role of nutrition in animal husbandry. \\n\\n### Response:\\nNutrition plays a crucial role in animal husbandry as it directly impacts the health, growth, and productivity of livestock. Properly balanced diets ensure optimal development and efficient utilization of nutrients for various purposes, such as milk and meat production.</s>\"},\n",
    "{ \"inputs\": \"<s>### Instruction:\\nElaborate on the challenges faced in modern animal husbandry practices. \\n\\n### Response:\\nModern animal husbandry faces challenges such as disease management, ethical concerns, and environmental impact. Balancing productivity with animal welfare and sustainability is an ongoing challenge for practitioners in the field.</s>\"},\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n"
   ],
   "metadata": {
    "id": "xDpSMxhjIqOL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "s6f4z8EYmcJ6",
    "ExecuteTime": {
     "end_time": "2024-02-10T11:33:45.375679700Z",
     "start_time": "2024-02-10T11:33:45.234329600Z"
    },
    "outputId": "66a32174-1f7d-431f-a387-6d10be9e61f4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset Size:  974\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"/content/data.json\") as f:  # Load the dataset from a JSON file\n",
    "    train_dataset = json.load(f)\n",
    "\n",
    "print(\"Dataset Size: \", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def divide_into_Batches(number, chunk_size):  # Define a function to divide a number into chunks of a given size\n",
    "    Batches = []\n",
    "    while number > 0:\n",
    "        if number >= chunk_size:\n",
    "            Batches.append(chunk_size)\n",
    "            number -= chunk_size\n",
    "        else:\n",
    "            Batches.append(number)\n",
    "            break\n",
    "\n",
    "    return Batches\n",
    "\n",
    "Batches = divide_into_Batches(len(train_dataset), 100)  # Divide the dataset into chunks of 100 samples each\n",
    "print(\"Batches size\")\n",
    "print(Batches)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0Hk_d2esaqv",
    "outputId": "7d4a6857-a07b-4261-b235-2bf2c578e09a",
    "ExecuteTime": {
     "end_time": "2024-02-10T11:33:49.093550900Z",
     "start_time": "2024-02-10T11:33:49.009625800Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches size\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 74]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> # 2. **LOAD BASE MODEL**"
   ],
   "metadata": {
    "collapsed": false,
    "id": "o1QGqDDEIqOP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now load Llama 2 7B - `NousResearch/Nous-Hermes-Llama2-13b` - using 4-bit quantization!"
   ],
   "metadata": {
    "collapsed": false,
    "id": "jkdjCswGIqOQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E0Nl5mWL0k2T",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "95c9facc-43d7-4ffe-a21b-2af95ddd7488",
    "ExecuteTime": {
     "end_time": "2024-02-10T12:08:11.947801Z",
     "start_time": "2024-02-10T12:08:10.895036Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Available Base Models\n",
      "\t <gradientai._base_model.BaseModel object at 0x7ddab568e710>\n",
      "\t <gradientai._base_model.BaseModel object at 0x7ddab568e6e0>\n",
      "\t <gradientai._base_model.BaseModel object at 0x7ddab568ee60>\n",
      "\t <gradientai._base_model.BaseModel object at 0x7ddab568ef20>\n",
      "\t <gradientai._base_model.BaseModel object at 0x7ddab568e9e0>\n",
      "\n",
      "Base Model Chosen : <gradientai._base_model.BaseModel object at 0x7ddab568efb0>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from gradientai import Gradient\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GradientLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "os.environ['GRADIENT_ACCESS_TOKEN'] = \"Dh8BfdF4J0CO7UBi7nXjZny7jh9breiK\"\n",
    "os.environ['GRADIENT_WORKSPACE_ID'] = \"345ce93a-40e9-4940-aa2e-fa76f1668fcd_workspace\"\n",
    "\n",
    "gradient =  Gradient()\n",
    "\n",
    "print(\"Available Base Models\")\n",
    "for i in gradient.list_models(only_base=True):\n",
    "    print(\"\\t\",i)\n",
    "\n",
    "base_model_id = \"NousResearch/Nous-Hermes-Llama2-13b\"\n",
    "base_model_name = \"nous-hermes2\"\n",
    "base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\") # base model Nous-Hermes-Llama2-13b\n",
    "\n",
    "print(\"\\nBase Model Chosen :\", base_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> # **3. CREATING A MODEL ADAPTER**\n",
    "\n",
    "* Adapters are small, lightweight modules inserted between\n",
    "existing layers of a pre-trained LLM. They act like \"add-ons\" that focus on learning task-specific information without modifying the core knowledge captured in the original model.\n",
    "\n",
    "* for our case we view an adapter is just a set of unfrozen weights that we are going to be training during the fintuning process.\n",
    "\n",
    "\n",
    "* The addapter server as the object that we are going to fin tune"
   ],
   "metadata": {
    "id": "1SVpsJc3569K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "our_finetune_model_name=\"Llama2-13b/Animal_Husbandry\"\n",
    "Fine_Tune__adapter = base_model.create_model_adapter(\n",
    "        name=our_finetune_model_name,\n",
    "        learning_rate=0.00005,\n",
    "        rank=8,\n",
    "\n",
    "    )\n",
    "\n",
    "# default hyperparameters Frozen\n",
    "hyperparameters = {\n",
    "                  \"block_size\": 1024,\n",
    "                  \"model_max_length\": 2048,\n",
    "                  \"padding\": \"right\",\n",
    "                  \"use_flash_attention_2\": False,\n",
    "                  \"disable_gradient_checkpointing\": False,\n",
    "                  \"logging_steps\": -1,\n",
    "                  \"evaluation_strategy\": \"epoch\",\n",
    "                  \"save_total_limit\": 1,\n",
    "                  \"save_strategy\": \"epoch\",\n",
    "                  \"auto_find_batch_size\": False,\n",
    "                  \"mixed_precision\": \"fp16\",\n",
    "                  \"epochs\": 3,\n",
    "                  \"batch_size\": 100,\n",
    "                  \"warmup_ratio\": 0.1,\n",
    "                  \"gradient_accumulation\": 1,\n",
    "                  \"optimizer\": \"adamw_torch\",\n",
    "                  \"scheduler\": \"linear\",\n",
    "                  \"weight_decay\": 0,\n",
    "                  \"max_grad_norm\": 1,\n",
    "                  \"seed\": 42,\n",
    "                  \"apply_chat_template\": False,\n",
    "                  \"quantization\": \"int4\",\n",
    "                  \"target_modules\": \"\",\n",
    "                  \"merge_adapter\": False,\n",
    "                  \"peft\": True,\n",
    "                  \"lora_r\": 16,\n",
    "                  \"lora_alpha\": 32,\n",
    "                  \"lora_dropout\": 0.05\n",
    "  }\n",
    "\n",
    "\n",
    "print(f\"Base model id                : {Fine_Tune__adapter._base_model_id}\")\n",
    "print(f\"Fine tune model Name         : { Fine_Tune__adapter.name}\")\n",
    "print(f\"Fine tune model adapter id   : {Fine_Tune__adapter.id}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Size of object in memory, in bytes.\", Fine_Tune__adapter.__format__.__sizeof__()) # Size of object in memory, in bytes.\n",
    "Fine_Tune__adapter.__dict__"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24r14ZbM1jIo",
    "outputId": "b6694a7f-dad8-43cb-bc36-1d9bd8450588",
    "ExecuteTime": {
     "end_time": "2024-02-10T12:08:29.630414900Z",
     "start_time": "2024-02-10T12:08:25.693892Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Base model id                : cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model\n",
      "Fine tune model Name         : Llama2-13b/Animal_Husbandry\n",
      "Fine tune model adapter id   : c26d5bd9-aa0a-4e3c-a715-eb124bd71d29_model_adapter\n",
      "\n",
      "\n",
      "\n",
      "Size of object in memory, in bytes. 56\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'_api_instance': <gradientai.openapi.client.api.models_api.ModelsApi at 0x7ddab568e380>,\n",
       " '_id': 'c26d5bd9-aa0a-4e3c-a715-eb124bd71d29_model_adapter',\n",
       " '_workspace_id': '345ce93a-40e9-4940-aa2e-fa76f1668fcd_workspace',\n",
       " '_async_semaphore': <asyncio.locks.Semaphore object at 0x7ddae82fa320 [unlocked, value:8]>,\n",
       " '_base_model_id': 'cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model',\n",
       " '_name': 'Llama2-13b/Animal_Husbandry'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "> # **4. FINE TUNING OUR ADAPTOR**\n",
    "\n",
    "For our case we will be performing Laura-based finetuning - this mean tat we are freezing like 99% of the layers and then finetuning an adapter on top of it.\n",
    "\n",
    "LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal with the problem of fine-tuning large-language models with billions of parameters\n",
    "\n",
    " LoRA proposes to freeze pre-trained model weights and inject trainable layers (rank-decomposition matrices) in each transformer block. This greatly reduces the number of trainable parameters and GPU memory requirements since gradients don't need to be computed for most model weights\n",
    "\n",
    "\n",
    "why LoRA finetuning:\n",
    "\n",
    "1. Faster Training: Since only the added task-specific layers are trained while the pre-trained model's parameters remain frozen, the fine-tuning process is generally faster compared to training a model from scratch\n",
    "2. Computation requirements are lower. We could create a full fine-tuned model in a 2080 Ti with 11 GB of VRAM!\n",
    "3. Trained weights are  much smaller. Because the original model is frozen and we inject new layers to be trained\n",
    "\n",
    " [for more info](https://huggingface.co/blog/lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "XX2C60VtdWx7",
    "outputId": "fcb4e710-9e0f-4b59-f83a-485898721e85",
    "ExecuteTime": {
     "end_time": "2024-02-10T13:22:18.810294400Z",
     "start_time": "2024-02-10T12:46:35.931015500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Model id:  d189f721-ae17-4545-a0ad-f95194e857f5_model_adapter\n",
      "================================================================\n",
      "\n",
      "Fine tuning . . .\n",
      "\n",
      "Fine-tuning the model, iteration 1\n",
      "Batch 1 range: 0 : 100\n",
      "\t Batch 1 Evaluation : number_of_trainable_tokens=8007 sum_loss=7168.5273\n",
      "Batch 2 range: 100 : 200\n",
      "\t Batch 2 Evaluation : number_of_trainable_tokens=8043 sum_loss=5564.6777\n",
      "Batch 3 range: 200 : 300\n",
      "\t Batch 3 Evaluation : number_of_trainable_tokens=9278 sum_loss=5753.655\n",
      "Batch 4 range: 300 : 400\n",
      "\t Batch 4 Evaluation : number_of_trainable_tokens=10454 sum_loss=7111.75\n",
      "Batch 5 range: 400 : 500\n",
      "\t Batch 5 Evaluation : number_of_trainable_tokens=13087 sum_loss=9291.248\n",
      "Batch 6 range: 500 : 600\n",
      "\t Batch 6 Evaluation : number_of_trainable_tokens=16332 sum_loss=12244.016\n",
      "Batch 7 range: 600 : 700\n",
      "\t Batch 7 Evaluation : number_of_trainable_tokens=18421 sum_loss=15847.841\n",
      "Batch 8 range: 700 : 800\n",
      "\t Batch 8 Evaluation : number_of_trainable_tokens=18931 sum_loss=17274.959\n",
      "Batch 9 range: 800 : 900\n",
      "\t Batch 9 Evaluation : number_of_trainable_tokens=13457 sum_loss=11424.976\n",
      "Batch 10 range: 900 : 974\n",
      "\t Batch 10 Evaluation : number_of_trainable_tokens=6669 sum_loss=5279.62\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our Model id:  {Fine_Tune__adapter.id}\")\n",
    "num_epochs = 1  # num_epochs is the number of times you fine-tune the model # more epochs tends to get better results, but you also run the risk of \"overfitting\"\n",
    "count = 0\n",
    "print(\"================================================================\\n\")\n",
    "print(\"Fine tuning . . .\\n\")\n",
    "while count < num_epochs:\n",
    "    print(f\"Fine-tuning the model, iteration {count + 1}\")\n",
    "    s = 0\n",
    "    n = 1\n",
    "    for Batch in Batches:\n",
    "        print(f\"Batch {n} range: {s} : {(s + Batch)}\")\n",
    "\n",
    "        # Try to fine-tune the model with the chunk of samples,\n",
    "        while True:\n",
    "            try:\n",
    "                metric = Fine_Tune__adapter.fine_tune(samples=train_dataset[s: s + Batch])\n",
    "                print(f\"\\t Batch {n} Evaluation :\", metric)\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "        s += Batch\n",
    "        n += 1\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> # **5. MODEL INFERENCE**"
   ],
   "metadata": {
    "id": "NGYDvRrJfAk7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.chains import LLMChain    # Import the LLMChain class for building LLM-based workflows\n",
    "from langchain.llms import GradientLLM   # Import the GradientLLM class for interacting with Gradient AI's API\n",
    "from langchain.prompts import PromptTemplate # Import the PromptTemplate class for defining how to prompt the LLM\n",
    "import gradientai\n",
    "import os # Import the os module for potential file system interactions"
   ],
   "metadata": {
    "id": "H1UnO0MeeTvU",
    "ExecuteTime": {
     "end_time": "2024-02-10T13:23:37.546254500Z",
     "start_time": "2024-02-10T13:23:37.514667700Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Fine_Tune__adapter_ID = \"d189f721-ae17-4545-a0ad-f95194e857f5_model_adapter\"\n",
    "#Fine_Tune__adapter_ID = Fine_Tune__adapter.id\n",
    "#  creating a GradientLLM object\n",
    "llm = GradientLLM(\n",
    "    model=Fine_Tune__adapter_ID,\n",
    "    model_kwargs=dict(\n",
    "        max_generated_token_count=128,\n",
    "        temperature = 0.7\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "id": "3Yglu4-EeRUb",
    "ExecuteTime": {
     "end_time": "2024-02-10T13:23:51.244675300Z",
     "start_time": "2024-02-10T13:23:51.175756400Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Formatting prompts**\n"
   ],
   "metadata": {
    "id": "wK-_OUIGiVVu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "template = \"\"\"### Instruction: {Instruction} \\n\\n### Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"Instruction\"])"
   ],
   "metadata": {
    "id": "eIwCIMlHiWey",
    "ExecuteTime": {
     "end_time": "2024-02-10T13:24:00.885524800Z",
     "start_time": "2024-02-10T13:24:00.824242800Z"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ],
   "metadata": {
    "id": "_xfjknhViZ5O",
    "ExecuteTime": {
     "end_time": "2024-02-10T13:24:03.937852700Z",
     "start_time": "2024-02-10T13:24:03.899758400Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Question = \"What is animal husbandry?\"\n",
    "\n",
    "Answer = llm_chain.run(Instruction=f\"{Question}\")\n",
    "print(Answer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZx0KRxMeQB4",
    "outputId": "ff52a072-129f-4c14-82c6-77519f1eb427",
    "ExecuteTime": {
     "end_time": "2024-02-10T13:24:13.390371200Z",
     "start_time": "2024-02-10T13:24:05.813162900Z"
    }
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Animal husbandry is the practice of raising livestock for commercial purposes, including meat, dairy, and other animal products.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "def Find_Instruction(input_string):\n",
    "    input_pattern = r'<s>### Instruction:\\n(.*?) \\n'\n",
    "    matches = re.findall(input_pattern, input_string, re.DOTALL)\n",
    "\n",
    "    # If there are matches, extract the first one\n",
    "    extracted_string = None\n",
    "    if matches:\n",
    "        extracted_string = matches[0]\n",
    "\n",
    "    return extracted_string\n",
    "\n",
    "question = train_dataset[0][\"inputs\"]\n",
    "question = Find_Instruction(question)\n",
    "print(\"question :\\n\\t\", question)\n",
    "Answer = llm_chain.run(Instruction=f\"{question}\")\n",
    "print(\"Answer : \\n\\t\", Answer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJOMhh0gjY9a",
    "outputId": "8d38a6a6-b358-4275-e063-80a249f9b7a6"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "question :\n",
      " Discuss the role of nutrition in animal husbandry.\n",
      "Answer : \n",
      " Nutrition plays a vital role in animal husbandry, affecting growth rates, reproductive success, and overall health in livestock.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    ">  #  **6. MODEL EVALUATION**\n",
    "\n",
    "Here we are using two popular automatic evaluation metrics to assess the performance of your LLM:\n",
    "\n",
    "* BLEU score: This metric calculates the n-gram precision between the generated response and the reference response\n",
    "\n",
    "* ROUGE score: This metric measures the overlap in word n-grams and longest common subsequences between the generated response and the reference response.\n",
    "\n",
    "Reasons for Choosing These Metrics:\n",
    "* Both BLEU and ROUGE are widely used in evaluating text generation tasks, making them well-established and understood metrics.\n",
    "* Both scores offer numerical values that can be easily compared and analyzed.\n",
    "\n",
    "BLEU and ROUGE scores are calculated to compare the generated response with the target response."
   ],
   "metadata": {
    "id": "zLF8kTHWOlWF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GradientLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T13:57:00.787085200Z",
     "start_time": "2024-02-10T13:57:00.739966400Z"
    },
    "id": "sm7RShvAIqOh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_rouge_scores(hypotheses, references):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def compute_bleu_score(target_response, llm_responses):\n",
    "    bleu_score = corpus_bleu([target_response.split()], [llm_responses.split()])  # Calculate BLEU score\n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "def Find_Instruction(input_pattern, input_string):\n",
    "    matches = re.findall(input_pattern, input_string, re.DOTALL)\n",
    "\n",
    "    # If there are matches, extract the first one\n",
    "    extracted_string = None\n",
    "    if matches:\n",
    "        extracted_string = matches[0]\n",
    "\n",
    "    return extracted_string\n",
    "\n",
    "\n",
    "def Evaluate(Sample=None, count=0):\n",
    "    print(\"\\n =================================== Evaluation =================================== \")\n",
    "    input_pattern = r'<s>### Instruction:\\n(.*?) \\n'\n",
    "    response_pattern = r'Response:\\n(.*?)</s>'\n",
    "    bleu_scoreS = []\n",
    "    rouge_scoreS = []\n",
    "\n",
    "    if count != 0:\n",
    "        iteration = count - 1\n",
    "    else:\n",
    "        iteration = count\n",
    "\n",
    "    while iteration >= 0:\n",
    "\n",
    "        input_query = Find_Instruction(input_pattern, Sample[iteration][\"inputs\"])\n",
    "        target_response = Find_Instruction(response_pattern, Sample[iteration][\"inputs\"])\n",
    "\n",
    "        if input_query and target_response is not None:\n",
    "            print(\"\\n ---------------------------------------------------------------\")\n",
    "            print(\"INPUT QUERY:\\n\", input_query)\n",
    "            print(\"\\nTARGET RESPONSE:\\n\", target_response)\n",
    "\n",
    "            llm_responses = llm_chain.run(Instruction=f\"{input_query}\")\n",
    "            print(\"\\nLLM RESPONSE:\\n\", llm_responses)\n",
    "\n",
    "            rouge_scores = compute_rouge_scores(llm_responses, target_response)\n",
    "\n",
    "            bleu_score = compute_bleu_score(target_response, llm_responses)\n",
    "            print(\"\\nBLEU Score:\", bleu_score)\n",
    "            print(\"ROUGE Scores:\")\n",
    "            print(\"\\tROUGE-1 F1 Score:\", rouge_scores[\"rouge-1\"][\"f\"])\n",
    "            print(\"\\tROUGE-2 F1 Score:\", rouge_scores[\"rouge-2\"][\"f\"])\n",
    "            print(\"\\tROUGE-L F1 Score:\", rouge_scores[\"rouge-l\"][\"f\"])\n",
    "            rouge_scoreS.append((rouge_scores[\"rouge-1\"][\"f\"], rouge_scores[\"rouge-2\"][\"f\"], rouge_scores[\"rouge-l\"][\"f\"]))\n",
    "            bleu_scoreS.append(bleu_score)\n",
    "\n",
    "\n",
    "        iteration -= 1\n",
    "\n",
    "    if count > 0:\n",
    "        rouge_scores1 = 0\n",
    "        rouge_scores2 = 0\n",
    "        rouge_scores3 = 0\n",
    "        bleu_scoreA = 0\n",
    "\n",
    "        for i in bleu_scoreS:\n",
    "            bleu_scoreA += i\n",
    "        for i in rouge_scoreS:\n",
    "            rouge_scores1 += i[0]\n",
    "            rouge_scores2 += i[1]\n",
    "            rouge_scores3 += i[2]\n",
    "\n",
    "        print(\"\\nAverageBLEU Score:\", bleu_scoreA)\n",
    "        print(f\"Average ROUGE Scores for {count} samples\")\n",
    "        print(\"\\tAverage ROUGE-1 F1 Score:\", rouge_scores1 / count)\n",
    "        print(\"\\tAverage ROUGE-2 F1 Score:\", rouge_scores2 / count)\n",
    "        print(\"\\tAverageROUGE-L F1 Score:\", rouge_scores3 / count)\n",
    "\n",
    "    print(\"\\n ---------------------------------------------------------------\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:00:53.823321600Z",
     "start_time": "2024-02-10T14:00:53.749991200Z"
    },
    "id": "2wmhQNKRIqOj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " =================================== Evaluation =================================== \n",
      "\n",
      " ---------------------------------------------------------------\n",
      "INPUT QUERY:\n",
      " How does technology contribute to advancements in animal husbandry?\n",
      "\n",
      "TARGET RESPONSE:\n",
      " Technology in animal husbandry includes innovations like automated feeding systems, precision breeding techniques, and health monitoring devices. These advancements enhance efficiency, reduce costs, and improve overall management practices.\n",
      "\n",
      "LLM RESPONSE:\n",
      " Technology contributes to advancements in animal husbandry by enhancing monitoring, efficiency, and decision-making through tools such as sensors, data analytics, and automation.\n",
      "\n",
      "BLEU Score: 0\n",
      "ROUGE Scores:\n",
      "\tROUGE-1 F1 Score: 0.2916666617447917\n",
      "\tROUGE-2 F1 Score: 0.08333332841145863\n",
      "\tROUGE-L F1 Score: 0.2916666617447917\n",
      "\n",
      " ---------------------------------------------------------------\n",
      "INPUT QUERY:\n",
      " Elaborate on the challenges faced in modern animal husbandry practices.\n",
      "\n",
      "TARGET RESPONSE:\n",
      " Modern animal husbandry faces challenges such as disease management, ethical concerns, and environmental impact. Balancing productivity with animal welfare and sustainability is an ongoing challenge for practitioners in the field.\n",
      "\n",
      "LLM RESPONSE:\n",
      " Modern animal husbandry faces challenges such as disease management, antibiotic resistance, and sustainable farming practices due to the sheer scale of production.\n",
      "\n",
      "BLEU Score: 0\n",
      "ROUGE Scores:\n",
      "\tROUGE-1 F1 Score: 0.43999999507200005\n",
      "\tROUGE-2 F1 Score: 0.31999999512800004\n",
      "\tROUGE-L F1 Score: 0.43999999507200005\n",
      "\n",
      " ---------------------------------------------------------------\n",
      "INPUT QUERY:\n",
      " Discuss the role of nutrition in animal husbandry.\n",
      "\n",
      "TARGET RESPONSE:\n",
      " Nutrition plays a crucial role in animal husbandry as it directly impacts the health, growth, and productivity of livestock. Properly balanced diets ensure optimal development and efficient utilization of nutrients for various purposes, such as milk and meat production.\n",
      "\n",
      "LLM RESPONSE:\n",
      " Nutrition plays a critical role in animal husbandry by influencing growth rates, reproductive performance, and overall health in livestock. A well-balanced diet, providing the essential nutrients at the appropriate levels, ensures optimal growth, reproduction, and overall health in animals.\n",
      "\n",
      "BLEU Score: 7.290245807398516e-232\n",
      "ROUGE Scores:\n",
      "\tROUGE-1 F1 Score: 0.3823529361807959\n",
      "\tROUGE-2 F1 Score: 0.13698629637830753\n",
      "\tROUGE-L F1 Score: 0.35294117147491355\n",
      "\n",
      "AverageBLEU Score: 7.290245807398516e-232\n",
      "Average ROUGE Scores for 3 samples\n",
      "\tAverage ROUGE-1 F1 Score: 0.37133986433252925\n",
      "\tAverage ROUGE-2 F1 Score: 0.18010653997258874\n",
      "\tAverageROUGE-L F1 Score: 0.3615359427639018\n",
      "\n",
      " ---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HEZRON WEKESA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\HEZRON WEKESA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\HEZRON WEKESA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "Evaluate(Sample=train_dataset, count=3)  # one sample evaluation"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:01:35.833398300Z",
     "start_time": "2024-02-10T14:00:58.779807400Z"
    },
    "id": "vu3hSvMkIqOl",
    "outputId": "8b080532-59fb-4761-9084-8d93f23c2078"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " > # **6. INTEGRATING  RETRIEVAL-AUGMENTED GENERATION TO OUR FINETUNED LLM**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "jgykmpx1IqOn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting gradient_haystack==0.2.0\n",
      "  Downloading gradient_haystack-0.2.0-py3-none-any.whl (11 kB)\n",
      "Collecting gradientai>=1.4.0 (from gradient_haystack==0.2.0)\n",
      "  Downloading gradientai-1.7.0-py3-none-any.whl (270 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m270.4/270.4 kB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting haystack-ai (from gradient_haystack==0.2.0)\n",
      "  Downloading haystack_ai-2.0.0b7-py3-none-any.whl (239 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.6/239.6 kB\u001B[0m \u001B[31m13.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting aenum>=3.1.11 (from gradientai>=1.4.0->gradient_haystack==0.2.0)\n",
      "  Downloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m137.6/137.6 kB\u001B[0m \u001B[31m17.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pydantic<2.0.0,>=1.10.5 (from gradientai>=1.4.0->gradient_haystack==0.2.0)\n",
      "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m28.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.0.7)\n",
      "Collecting boilerpy3 (from haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
      "Collecting haystack-bm25 (from haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading haystack_bm25-1.0.2-py2.py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.1.3)\n",
      "Collecting lazy-imports (from haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (10.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.2.1)\n",
      "Collecting openai>=1.1.0 (from haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m226.7/226.7 kB\u001B[0m \u001B[31m32.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.5.3)\n",
      "Collecting posthog (from haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading posthog-3.4.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.1/41.1 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (6.0.1)\n",
      "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (8.2.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (4.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.9/75.9 kB\u001B[0m \u001B[31m10.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-bm25->haystack-ai->gradient_haystack==0.2.0) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai->gradient_haystack==0.2.0) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack==0.2.0) (2023.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (2.31.0)\n",
      "Collecting monotonic>=1.5 (from posthog->haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog->haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.9/76.9 kB\u001B[0m \u001B[31m10.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai->gradient_haystack==0.2.0) (3.3.2)\n",
      "Installing collected packages: monotonic, aenum, pydantic, lazy-imports, haystack-bm25, h11, boilerpy3, backoff, posthog, httpcore, gradientai, httpx, openai, haystack-ai, gradient_haystack\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.6.1\n",
      "    Uninstalling pydantic-2.6.1:\n",
      "      Successfully uninstalled pydantic-2.6.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "lida 0.0.10 requires fastapi, which is not installed.\n",
      "lida 0.0.10 requires kaleido, which is not installed.\n",
      "lida 0.0.10 requires python-multipart, which is not installed.\n",
      "lida 0.0.10 requires uvicorn, which is not installed.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\n",
      "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed aenum-3.1.15 backoff-2.2.1 boilerpy3-1.0.7 gradient_haystack-0.2.0 gradientai-1.7.0 h11-0.14.0 haystack-ai-2.0.0b7 haystack-bm25-1.0.2 httpcore-1.0.2 httpx-0.26.0 lazy-imports-0.3.1 monotonic-1.6 openai-1.12.0 posthog-3.4.0 pydantic-1.10.14\n"
     ]
    }
   ],
   "source": [
    "!pip install gradient_haystack==0.2.0"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:21:33.396982Z",
     "start_time": "2024-02-10T14:20:13.510315900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de5LGSqTIqOo",
    "outputId": "fd1475ca-171e-46f2-eeb2-5b4c4c22d69d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gradient_haystack.embedders.gradient_document_embedder import GradientDocumentEmbedder\n",
    "from gradient_haystack.embedders.gradient_text_embedder import GradientTextEmbedder\n",
    "from gradient_haystack.generator.base import GradientGenerator\n",
    "from haystack import Document, Pipeline\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory.document_store import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory.embedding_retriever import InMemoryEmbeddingRetriever\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "import os"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:11:15.061718Z",
     "start_time": "2024-02-10T14:11:15.013776400Z"
    },
    "id": "bOIAiWa3IqOr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "os.environ['GRADIENT_ACCESS_TOKEN'] = \"Dh8BfdF4J0CO7UBi7nXjZny7jh9breiK\"\n",
    "os.environ['GRADIENT_WORKSPACE_ID'] = \"345ce93a-40e9-4940-aa2e-fa76f1668fcd_workspace\"\n",
    "\n",
    "\n",
    "fine_tuned_Model_Id = \"d189f721-ae17-4545-a0ad-f95194e857f5_model_adapter\""
   ],
   "metadata": {
    "id": "5fkebnKtJqVl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "\n",
    "document_embedder = GradientDocumentEmbedder(\n",
    "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
    "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
    ")\n",
    "\n",
    "with open(\"/content/Raw_Text_Data.txt\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "docs = [\n",
    "    Document(content=text_data)\n",
    "]\n",
    "\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "indexing_pipeline.add_component(instance=writer, name=\"writer\")\n",
    "indexing_pipeline.connect(\"document_embedder\", \"writer\")\n",
    "indexing_pipeline.run({\"document_embedder\": {\"documents\": docs}})\n",
    "\n",
    "text_embedder = GradientTextEmbedder(\n",
    "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
    "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
    ")\n",
    "\n",
    "generator = GradientGenerator(\n",
    "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
    "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
    "    model_adapter_id=fine_tuned_Model_Id,\n",
    "    max_generated_token_count=350,\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-10T14:11:17.099725600Z",
     "start_time": "2024-02-10T14:11:16.989421700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liNzhUtuIqOs",
    "outputId": "0d21f801-fd11-4b13-9057-4a92938625f3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are helpful assistant ment to answer questions relating to animal husbandry. Answer the query, based on the\n",
    "content in the documents. if you dont know the answer say you don't know.\n",
    "{{documents}}\n",
    "Query: {{query}}\n",
    "\\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "prompt_builder = PromptBuilder(template=prompt)\n",
    "\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(instance=text_embedder, name=\"text_embedder\")\n",
    "rag_pipeline.add_component(instance=retriever, name=\"retriever\")\n",
    "rag_pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
    "rag_pipeline.add_component(instance=generator, name=\"generator\")\n",
    "rag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
    "rag_pipeline.connect(\"generator.replies\", \"answer_builder.replies\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
    "rag_pipeline.connect(\"text_embedder\", \"retriever\")\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
    "\n",
    "\n",
    "def LLM_Run(question):\n",
    "    result = rag_pipeline.run(\n",
    "        {\n",
    "            \"text_embedder\": {\"text\": question},\n",
    "            \"prompt_builder\": {\"query\": question},\n",
    "            \"answer_builder\": {\"query\": question}\n",
    "        }\n",
    "    )\n",
    "    return result[\"answer_builder\"][\"answers\"][0].data"
   ],
   "metadata": {
    "id": "gDm7xiU_IqOt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Query = \"When is diarrhoea very risky?\"\n",
    "print(LLM_Run(Query))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slM9Ez96JwVJ",
    "outputId": "1bfc7a2e-42f8-424c-ff68-c68c0c12919a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Diarrhoea is very risky when it is caused by a viral infection, as it can lead to severe dehydration and electrolyte imbalances in the affected animal.\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "collapsed_sections": [
    "jgykmpx1IqOn"
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
